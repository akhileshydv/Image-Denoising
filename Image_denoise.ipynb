{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_denoise.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rnRouk8_VJDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9f77f57-0c1c-4d46-97a4-6180b44a208e"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvi3iiyaFhP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a70ccd4-a8a5-480e-a987-a6af5e34c247"
      },
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXojJKzAFmZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgGdNXiNHm-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "562a0086-8707-4b12-d1a1-d789904285ba"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install imgaug"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting imgaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.19.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (0.5.2)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (4.0.0)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image>=0.11.0->imgaug) (0.45.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2018.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.5.3)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.3.0)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Running setup.py bdist_wheel for imgaug ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "Successfully installed imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XC5NmMOVFo_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imgaug import augmenters as iaa\n",
        "seq = iaa.Sequential([iaa.SaltAndPepper(0.2)])\n",
        "\n",
        "train_x_aug = seq.augment_images(train_x)\n",
        "val_x_aug = seq.augment_images(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M537Z25cFrHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 28, 28, 1)\n",
        "val_x = val_x.reshape(-1, 28, 28, 1)\n",
        "\n",
        "train_x_aug = train_x_aug/255.\n",
        "val_x_aug = val_x_aug/255.\n",
        "\n",
        "train_x_aug = train_x_aug.reshape(-1, 28, 28, 1)\n",
        "val_x_aug = val_x_aug.reshape(-1, 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pa6E8BCsJAYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8h8-w_bFvhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # this is our input placeholder\n",
        "    input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "    # \"encoded\" is the encoded representation of the input\n",
        "    encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "    encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "    encoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "    # \"decoded\" is the lossy reconstruction of the input\n",
        "    decoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    decoded = UpSampling2D((2, 2))(decoded)\n",
        "    decoded = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)\n",
        "    decoded = UpSampling2D((2, 2))(decoded)\n",
        "    decoded = Conv2D(64, (3, 3), activation='relu')(decoded)\n",
        "    decoded = UpSampling2D((2, 2))(decoded)\n",
        "    decoded = Conv2D(1, (3, 3), padding='same')(decoded)\n",
        "\n",
        "    # this model maps an input to its reconstruction\n",
        "    autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1f7XZaFJGGqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "1bac7d71-dafd-40ff-9df0-4c892649b40d"
      },
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 16)          4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 32)          4640      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
            "=================================================================\n",
            "Total params: 49,761\n",
            "Trainable params: 49,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J2-P6qnUGLLi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtoarrF4GNuZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80SI4TfvGQcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38gssTzIGT8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13379
        },
        "outputId": "eaf3fc41-4256-443b-b638-103a1b5b2eaf"
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "   train_history = autoencoder.fit(train_x_aug, train_x, epochs=500, batch_size=2048, validation_data=(val_x_aug, val_x), callbacks=[estop])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0977 - val_loss: 0.0535\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0447 - val_loss: 0.0373\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0342 - val_loss: 0.0309\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0287 - val_loss: 0.0266\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0255 - val_loss: 0.0243\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0238 - val_loss: 0.0230\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0225 - val_loss: 0.0220\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0217 - val_loss: 0.0213\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0210 - val_loss: 0.0206\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0204 - val_loss: 0.0201\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0199 - val_loss: 0.0198\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0194 - val_loss: 0.0192\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0192 - val_loss: 0.0189\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0187 - val_loss: 0.0185\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0185 - val_loss: 0.0182\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0183 - val_loss: 0.0182\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0179 - val_loss: 0.0177\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0176 - val_loss: 0.0176\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0176 - val_loss: 0.0175\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0173 - val_loss: 0.0172\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0172 - val_loss: 0.0170\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0169 - val_loss: 0.0168\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0169 - val_loss: 0.0167\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0167 - val_loss: 0.0166\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0165 - val_loss: 0.0166\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0165 - val_loss: 0.0163\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0162 - val_loss: 0.0167\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0162 - val_loss: 0.0161\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0160 - val_loss: 0.0161\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0161 - val_loss: 0.0159\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0158 - val_loss: 0.0158\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0158 - val_loss: 0.0162\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0157 - val_loss: 0.0156\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0155 - val_loss: 0.0157\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0156 - val_loss: 0.0156\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0154 - val_loss: 0.0154\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0152 - val_loss: 0.0154\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0153 - val_loss: 0.0153\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0151 - val_loss: 0.0151\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0152 - val_loss: 0.0150\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0150 - val_loss: 0.0150\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0149 - val_loss: 0.0149\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0148 - val_loss: 0.0148\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0147 - val_loss: 0.0149\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0147 - val_loss: 0.0148\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0147 - val_loss: 0.0146\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0145 - val_loss: 0.0146\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0145 - val_loss: 0.0144\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0144 - val_loss: 0.0144\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0143 - val_loss: 0.0144\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0143 - val_loss: 0.0142\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0145 - val_loss: 0.0144\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0141 - val_loss: 0.0141\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0140 - val_loss: 0.0141\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0140 - val_loss: 0.0140\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0143 - val_loss: 0.0140\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0139 - val_loss: 0.0139\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0138 - val_loss: 0.0143\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0138 - val_loss: 0.0138\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0139 - val_loss: 0.0139\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0138 - val_loss: 0.0138\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0136 - val_loss: 0.0139\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0137 - val_loss: 0.0137\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0136 - val_loss: 0.0141\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0137 - val_loss: 0.0136\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0135 - val_loss: 0.0138\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0135 - val_loss: 0.0135\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0136 - val_loss: 0.0134\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0134 - val_loss: 0.0137\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0133 - val_loss: 0.0133\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0133 - val_loss: 0.0133\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0132 - val_loss: 0.0133\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0132 - val_loss: 0.0132\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0134 - val_loss: 0.0131\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0131 - val_loss: 0.0131\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0131 - val_loss: 0.0132\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0131 - val_loss: 0.0131\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0131 - val_loss: 0.0132\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0129 - val_loss: 0.0130\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0130 - val_loss: 0.0130\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0130 - val_loss: 0.0129\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0129 - val_loss: 0.0132\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0128 - val_loss: 0.0128\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0129 - val_loss: 0.0130\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0128 - val_loss: 0.0128\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0128 - val_loss: 0.0130\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0128 - val_loss: 0.0129\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0127 - val_loss: 0.0127\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0127 - val_loss: 0.0127\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0127 - val_loss: 0.0128\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0126 - val_loss: 0.0128\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0127 - val_loss: 0.0128\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0125 - val_loss: 0.0125\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0125 - val_loss: 0.0125\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0125 - val_loss: 0.0127\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0125 - val_loss: 0.0126\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0124 - val_loss: 0.0126\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0124 - val_loss: 0.0124\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0124 - val_loss: 0.0124\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0124 - val_loss: 0.0124\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0123 - val_loss: 0.0123\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0123 - val_loss: 0.0124\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0122 - val_loss: 0.0123\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0126 - val_loss: 0.0125\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0123 - val_loss: 0.0128\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0121 - val_loss: 0.0122\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0122 - val_loss: 0.0121\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0120 - val_loss: 0.0124\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0120 - val_loss: 0.0124\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0123 - val_loss: 0.0121\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0119 - val_loss: 0.0120\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0119 - val_loss: 0.0124\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0120 - val_loss: 0.0119\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0120\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0118 - val_loss: 0.0126\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0118\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0118 - val_loss: 0.0117\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0116 - val_loss: 0.0117\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0116 - val_loss: 0.0116\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0116 - val_loss: 0.0118\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0115 - val_loss: 0.0118\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0117 - val_loss: 0.0116\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0116 - val_loss: 0.0119\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0115 - val_loss: 0.0118\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0116 - val_loss: 0.0115\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0115 - val_loss: 0.0119\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0115 - val_loss: 0.0114\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0115 - val_loss: 0.0114\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0118\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0115\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0112 - val_loss: 0.0115\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0112 - val_loss: 0.0112\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0112 - val_loss: 0.0112\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0111 - val_loss: 0.0113\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0112 - val_loss: 0.0116\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0111 - val_loss: 0.0121\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0112 - val_loss: 0.0112\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0110 - val_loss: 0.0113\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0113 - val_loss: 0.0112\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0112 - val_loss: 0.0116\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - val_loss: 0.0111\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0109 - val_loss: 0.0111\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0109 - val_loss: 0.0112\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0110 - val_loss: 0.0111\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0110 - val_loss: 0.0109\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0109 - val_loss: 0.0110\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0109 - val_loss: 0.0110\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0113\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0113\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - val_loss: 0.0109\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0110\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0114\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0109 - val_loss: 0.0108\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0110\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0108 - val_loss: 0.0111\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0110 - val_loss: 0.0108\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0108\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0108\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0106 - val_loss: 0.0110\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0105 - val_loss: 0.0108\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0106 - val_loss: 0.0106\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0105 - val_loss: 0.0107\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0105 - val_loss: 0.0108\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0106\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0107\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0107\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0106 - val_loss: 0.0106\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0107 - val_loss: 0.0106\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0106\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0106 - val_loss: 0.0106\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0108 - val_loss: 0.0120\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0107 - val_loss: 0.0105\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0107\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0105 - val_loss: 0.0108\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0105\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0107\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0106\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0105\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0108\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0110\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0104\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0109\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0104\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 302/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 303/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 304/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0103\n",
            "Epoch 305/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0105\n",
            "Epoch 306/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 307/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 308/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 309/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0106\n",
            "Epoch 310/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0104 - val_loss: 0.0103\n",
            "Epoch 311/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 312/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 313/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 314/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0104 - val_loss: 0.0103\n",
            "Epoch 315/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 316/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0105\n",
            "Epoch 317/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 318/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 319/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 320/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 321/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0103 - val_loss: 0.0103\n",
            "Epoch 322/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 323/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 324/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 325/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0105 - val_loss: 0.0102\n",
            "Epoch 326/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 327/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 328/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 329/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0105\n",
            "Epoch 330/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 331/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0105\n",
            "Epoch 332/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 333/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 334/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0107\n",
            "Epoch 335/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 336/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 337/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 338/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 339/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0112\n",
            "Epoch 340/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0103 - val_loss: 0.0101\n",
            "Epoch 341/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 342/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 343/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0104\n",
            "Epoch 344/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 345/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 346/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 347/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 348/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 349/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 350/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 351/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 352/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 353/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 354/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0102 - val_loss: 0.0103\n",
            "Epoch 355/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 356/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 357/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 358/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0105\n",
            "Epoch 359/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 360/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 361/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 362/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0105\n",
            "Epoch 363/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 364/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0101 - val_loss: 0.0103\n",
            "Epoch 365/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 366/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 367/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 368/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 369/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 370/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 371/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 372/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 373/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 374/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 375/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0105\n",
            "Epoch 376/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 377/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 378/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0101\n",
            "Epoch 379/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 380/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 381/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0100\n",
            "Epoch 382/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 383/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 384/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 385/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 386/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0101\n",
            "Epoch 387/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 388/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 389/500\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 390/500\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0099 - val_loss: 0.0101\n",
            "Epoch 391/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0100 - val_loss: 0.0104\n",
            "Epoch 392/500\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 00392: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WgLuffBwGXEj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pzndrmqRGZty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0b8b0d7f-6f5f-44ef-fb8d-f30daa459034"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc2ca44acc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEcdJREFUeJzt3W2sVdWdx/HvFQQFHwEBwSp6xb+U\nezHxIYDIlLbU2zGiiUCMIcYA0TqptclIjJ2+EHgxHTGEiQ9pUou1YdJEjVqwLVplJuVFX0BUxnsb\nXYOmEr2glCcBBQQ88+Kec3LOYe+1zt3nEdfvkxDP3uvuff6ey4+9z157r9WRy+UQkW+2M1pdgIg0\nnoIuEgEFXSQCCrpIBBR0kRjkcrmG/wFypX96e3tzleva5Y9qU22na12+DHZk7V4zszXAjPyb/NQ5\ntzXtZzs6OsreJJfL0dHRkel9G021ZaPaBq/edeVyudSdZTp1N7PvAJOdczOBpcATGWsTkSbI+h39\n+8DvAZxz7wEXmtl5datKROpqaMbtxgNvlSz/I7/uYNIP9/b20tXVVbaune/IU23ZqLbBa1ZdWYNe\nyftFo7u7u2y5Xb8zgWrLSrUNXgO+o6e2ZT1138nAEbxgArAr475EpMGyBv3PwAIAM7sW2OmcO1S3\nqkSkrjIF3Tn3V+AtM/srA1fcf1zXqkSkrjL3ow/qTdSPXheqLZt2ra3t+9FF5PSioItEQEEXiYCC\nLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKg\noItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6SASGZtnI\nzOYALwJ/y6/qdc79pF5FiUh9ZQp63l+ccwvqVomINIxO3UUiUMsR/dtmtgEYBaxwzr2R9oO9vb10\ndXWVrcvlcjW8dWOptmxU2+A1q66OLG9kZhOBm4AXgCuA/wGudM59lfgmHR1lb5LL5ejo6Bh8tU2g\n2rJRbYNX77pyuVzqzjIFvZKZbQHudM79PfFNFPS6UG3ZtGttzQx6pu/oZrbIzJblX48HxgH92coT\nkUbL+h19A/A7M7sdGAb8S9ppu4i0Xl1O3YNvolP3ulBt2bRrbW1/6i4ipxcFXSQCCrpIBBR0kQgo\n6CIRqOUWWJGWGDJkiLf966+/PmVd4ep2rb1Mw4cP97YfO3bM237llVemLn/wwQfZCwvQEV0kAgq6\nSAQUdJEIKOgiEVDQRSKgoItEQEEXiYD60SMVemoq1J7UV11q4sSJqW0zZ870brtx40Zv+xdffOFt\nT1KvpzRD/eQh8+fPT11+7LHHatq3j47oIhFQ0EUioKCLREBBF4mAgi4SAQVdJAIKukgE1I8uiUL9\n5CGzZ89ObZs+fbp32wkTJnjbn3jiiUw11cPYsWO97T09Pd72gwcPepcbRUd0kQgo6CIRUNBFIqCg\ni0RAQReJgIIuEgEFXSQC6kePVGhs9BMnTnjbr7/+eu+6KVOmpG772Wefefc9efJkb/srr7zibd+3\nb98p69auXQvA2Wef7d12x44d3vbRo0d728877zxv+yeffFK23NnZ6f35eqkq6GbWBawH1jjnnjKz\nbwHrgCHALuBu51xtT+SLSMMET93NbCTwJLCpZPVK4Gnn3GzgA2BJY8oTkXqo5jv6MeAWYGfJujnA\nhvzrV4G59S1LROqpo9qxtMxsObAnf+q+2zk3Nr++E1jnnLsxbdu+vr5cV1dXPeoVkXSpA/3V42Kc\nfxRBoLu7u2w5l8sFBx9slVhqGzrU/6sf7MW4rVu3csMNNxSX582bl7rtkSNHvPv2DSwJcMkll3jb\nKy/GLVmyhGeffRZor4txDz30EKtXry4uL1u2zLttiO+gnbV77bCZFT6xiZSf1otIm8ka9DeBwji1\n84HX6lOOiDRC8NTdzK4DVgOTgONmtgBYBDxnZj8CdgC/bWSRMnhnnOH/Nzx0aj5y5Ehv+8KFC73r\nfOOfn3XWWd59n3vuud720NeXpP/3wrrQtlOnTvW2f/zxx972/fv3e9srvzKFvkLVS/BdnHNvMXCV\nvdIP6l6NiDSEboEViYCCLhIBBV0kAgq6SAQUdJEI6DHVAF93TOj24VAXV2j7ULvvUdOTJ096tw25\n//77ve2ffvqpd93Ro0dTt500aZJ336Hut9BjrkmfS+HzCA1jHZqS+auvvvK2h+6MGz58eOrPh7o0\ns0wXXaAjukgEFHSRCCjoIhFQ0EUioKCLREBBF4mAgi4SgW98P3roscRa+7J9ap16OKk/uHRdLX3l\nd911l7d9/Pjx3va33377lHWl/ehnnnlm6rYXXHCBd9979+71ticN51xqzJgxp6w7fvw4EH4ENjQM\ndkjo3okRI0aULZf2nYeGud62bVv2ujJvKSKnDQVdJAIKukgEFHSRCCjoIhFQ0EUioKCLROAb349e\nSz84+PtFQ32moX7uUG1J21fbd7548WJvu5l520PDGif1VZeu892/EJotpb+/39se6gtPun+hsO7L\nL7/0bht6Fr7W+zJ8enp6vO3qRxcRLwVdJAIKukgEFHSRCCjoIhFQ0EUioKCLROC06EcP9Vf7hPo1\nk/pFS9/P90x5rc+bh0yYMMG77o477kjdNtRXvX37dm/7Oeec422vHJ8cyvugR48enbptaGz00O+s\n8pnuahSejw/dh+Cb7rma7UNjr1f+nTl06FDx9axZs7zb1qKqoJtZF7AeWOOce8rMngOuAwojBDzu\nnPtjY0oUkVoFg25mI4EngU0VTT9zzv2hIVWJSF1Vc058DLgF2NngWkSkQTqqvTfXzJYDe0pO3ccD\nw4DdwAPOuT1p2/b19eW6urpqr1ZEfFJvxM96MW4dsNc5t83MHgGWAw+k/XB3d3fZci6XCz4cUKqZ\nF+NOnjxZNkBgoy+4+VRejOvv72fixInF5VouxoUGWAxdjBs2bFjZ8qpVq3j44YeLy76LcUkPxJTa\nvXu3tz20faV7772XZ555Bgj/XRo61B+J0MCVlZ9LpdLfy9KlS1m7dm1xeezYsd5tb7vtNm+77+96\npqA750q/r28AfpllPyLSHJkOlWb2kpldkV+cA/TVrSIRqbtqrrpfB6wGJgHHzWwBA1fhnzezL4HD\ngPfh51rHJ2/k6XPS6U6173fRRRd52y+77DJv+9VXX+1tv/jii09Zt2jRouJrX3/0wYMHvfsOja0e\nmuc7adz2Cy+8sPg6qZ+9IPT5hj4335jxAAcOHEhtK4zvniZUW+jU/8iRI972yiyU/g5L+9STTJ06\n1dvuEwy6c+4tBo7alV7K/K4i0lS6BVYkAgq6SAQUdJEIKOgiEVDQRSLQlMdUaxm2GGDcuHGpbaGu\nmNJpaattv/XWW4uvfXeYXX755d59hx6nDHX1HD58+JR1pd0xvq6e888/37vv0J1zJ06c8LYn/b+V\ndnv5hlUOPQoaurts165d3vak//fCHZCh38n+/fu97aE7Bku7GJNUPsZa+vcvNFW1727DEB3RRSKg\noItEQEEXiYCCLhIBBV0kAgq6SAQUdJEItMVwz3PnzvW2Jw17XBDqiw6N2pHUFz158uTia99ji6H3\nDj12GOqTTepXLV3nG6XH95gohPuLQ49jJtVeui7p0eSC0JDIoc/t888/97Yn/c5Dn0e1Qp9b6DHX\nyvsXSn+HofsHQvc2+OiILhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEoCn96DfffLN33dKlS73b\nv//++6ltoWeTQ8MeJ/X3lvbj+oZU9vUVVyPUX5zUr9rf31987XumPzRcc2imnNDz6kn9xaXrfEMy\nh5679o0/AOFhj5PeuzB2QK2/s9A9AKHn3Y8ePVq2XDrzTmjfoRlsfHREF4mAgi4SAQVdJAIKukgE\nFHSRCCjoIhFQ0EUi0JR+9C1btnjXzZgxw7t9d3d3atusWbOyF0byM74LFiwovvb1de/bt8+771B7\n6LnqpH70UaNGFV/7+sJDY4Cbmbc91B+c1E9/0003FV8nTUddcM0113j3/e6773rbP/roI2970vgG\n1157LRB+Lt1XdzVCz4yX3gcB5fMShO75CI1f4FNV0M1sFTA7//O/ALYC64AhwC7gbuecf1R+EWmZ\n4Km7mX0X6HLOzQR+CPwnsBJ42jk3G/gAWNLQKkWkJtV8R98MLMy/PgCMBOYAG/LrXgX8Y0GJSEt1\nDOY7iZndx8ApfI9zbmx+XSewzjl3Y9p27733Xm7KlCm11ioifqkXbaq+GGdmtwNLgZuB7dXsvODG\nG8v/Ddi/f3/ZZHQPPvigd3vfxbjQpHYhlRdPenp6eP3114vL7XQxbsWKFTz66KPF5Xa6GDdt2rSy\ni2jtdDFuxIgRxUkf2+liXGdnJx9++GFxOXQxbskS/zfkd955J7Wtqu41M+sBfg78s3Puc+CwmRUe\nb5oI7KxmPyLSGsEjupmdDzwOzHXOFQ5RbwLzgf/K//c13z4OHDjgXbdy5cqqC64U6nKYPn26t/2q\nq64qW+7p6WH9+vXF5cqzkVKTJk3y7nvatGne9tCUzklH7IULFxZf+44+oWGHQ2cbvb293vY33nij\nbPnll19m+fLlxeWNGzemblv5qGa9bdiwoWx53rx5bNq0CYBLL73Uu+2ePXu87aFHi0PtpUf8zs5O\nNm/eXFwOTSe9fft2b7tPNafudwJjgBdKTvfuAX5tZj8CdgC/zVyBiDRcMOjOuV8Bv0po+kH9yxGR\nRtAtsCIRUNBFIqCgi0RAQReJgIIuEoFB3QKb+U06OsreJJfLBYcbbhXVlo1qG7x615XL5VJ3piO6\nSAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXiYCC\nLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhKBaqZNxsxWAbPzP/8L4DbgOmBv/kce\nd879sSEVikjNgkE3s+8CXc65mWY2GngH+G/gZ865PzS6QBGpXTVH9M3AlvzrA8BIYEjDKhKRuhvU\nlExmdh8Dp/AngfHAMGA38IBzbk/adn19fbmurq4aSxWRgNQpmaoOupndDvwbcDNwPbDXObfNzB4B\nLnHOPZD6Jpp7rS5UWzbtWlsz516r9mJcD/Bz4IfOuc+BTSXNG4Bf1lShiDRUsHvNzM4HHgdudc7t\ny697ycyuyP/IHKCvYRWKSM2qOaLfCYwBXjCzwrrfAM+b2ZfAYWBxY8oTkXrQ/OgVVFs2qm3wND+6\niNSVgi4SAQVdJAIKukgEFHSRCCjoIhFQ0EUioKCLREBBF4mAgi4SAQVdJAIKukgEFHSRCCjoIhFo\nymOqItJaOqKLREBBF4mAgi4SAQVdJAIKukgEFHSRCCjoIhGoaqaWejKzNcAMIAf81Dm3tdk1JDGz\nOcCLwN/yq3qdcz9pXUVgZl3AemCNc+4pM/sWsI6BSS53AXc75461SW3P0SZTaSdM872VNvjcWjn9\neFODbmbfASbnp2CeAjwLzGxmDQF/cc4taHURAGY2EniS8umvVgJPO+deNLN/B5bQgumwUmqDNphK\nO2Wa7020+HNr9fTjzT51/z7wewDn3HvAhWZ2XpNrOF0cA24Bdpasm8PAXHcArwJzm1xTQVJt7WIz\nsDD/ujDN9xxa/7kl1dW06cebfeo+HnirZPkf+XUHm1xHmm+b2QZgFLDCOfdGqwpxzp0ATpRMgwUw\nsuSUczdwcdMLI7U2gAfM7F+pYirtBtZ2Evgiv7gU+BPQ0+rPLaWukzTpM2v1xbh2midnO7ACuB24\nB1hrZsNaW5JXO312MPAd+BHn3PeAbcDyVhaTn+Z7KVA5nXdLP7eKupr2mTX7iL6TgSN4wQQGLo60\nnHOuH3g+v/ihmX0KTAT+3rqqTnHYzM52zh1hoLa2OXV2zrXNVNqV03ybWVt8bq2cfrzZR/Q/AwsA\nzOxaYKdz7lCTa0hkZovMbFn+9XhgHNDf2qpO8SYwP/96PvBaC2sp0y5TaSdN800bfG6tnn686Y+p\nmtl/AP8EfA382Dn3v00tIIWZnQv8DrgAGMbAd/Q/tbCe64DVwCTgOAP/6CwCngPOAnYAi51zx9uk\ntieBR4DiVNrOud0tqO0+Bk6B/69k9T3Ar2nh55ZS128YOIVv+Gem59FFItDqi3Ei0gQKukgEFHSR\nCCjoIhFQ0EUioKCLREBBF4nA/wNtYzDYm8UacwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc33e1ce8d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_nXteJ2oGc_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "aa8d64b2-dbd6-47c9-e463-5b2892ee1786"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(val_x_aug[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc2b2faeb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc2ca3dd898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}